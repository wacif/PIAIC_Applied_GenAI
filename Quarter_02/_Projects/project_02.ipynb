{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojzqqtyw23kV"
      },
      "source": [
        "# **Project 02: LangChain RAG Project**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR95GMJ83OVN"
      },
      "source": [
        "####  **Task**\n",
        "Create a Google Colab Notebook that integrates a RAG workflow, leveraging the Google Gemini Flash API and Pinecone for vector storage and retrieval. Your system should:\n",
        "- **Load and Chunk Documents:** Demonstrate how to load a document (e.g., `documents.txt`), split it into smaller chunks, and embed these chunks using Gemini embedding.\n",
        "- **Store and Retrieve from Pinecone:** Set up Pinecone, create an index, and store embedding. Show how your system retrieves context for user queries.\n",
        "- **Integrate Gemini Flash LLM:** Use the Gemini Flash model in a Retrieval QA chain to answer user questions based on the retrieved context.\n",
        "- **Experiment with Parameters:** Fine-tune your RAG system by adjusting chunk size, overlap, temperature, or other relevant parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjF3bKBW1pDY"
      },
      "source": [
        "##  **Installing Required Libraries**\n",
        "\n",
        "This cell installs all the necessary Python libraries required for the project:\n",
        "\n",
        "- `langchain-pinecone`: Provides integration between LangChain and Pinecone.\n",
        "- `pinecone-notebooks`: Contains utilities for working with Pinecone in notebooks.\n",
        "- `langchain-google-genai`: For integrating LangChain with Google Gemini's API.\n",
        "- `langchain-community`: Adds community-supported integrations.\n",
        "- `pypdf`: For working with PDFs, including text extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJa8GMP0Y1rN",
        "outputId": "dfd388a9-80fc-491e-ef5f-3ee79bbca0b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/2.5 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m2.0/2.5 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-pinecone pinecone-notebooks langchain-google-genai langchain-community pypdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2rl4HcO19R8"
      },
      "source": [
        "##  **Load Environment Variables**\n",
        "\n",
        "- This cell sets up the environment variables required for authentication:\n",
        "  - `GOOGLE_API_KEY`: For authenticating with the Google Gemini API.\n",
        "  - `PINECONE_API_KEY`: For accessing Pinecone services.\n",
        "- `userdata.get()` ensures secure fetching of keys in Google Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NfFgr3ygrZ2W"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7ZhdImr2P9M"
      },
      "source": [
        "##  **Initialize Pinecone**\n",
        "\n",
        "- Purpose: Set up the Pinecone index for vector storage.\n",
        "- Checks if an index named `project-2-index` exists.\n",
        "  - If not, creates the index with:\n",
        "    - `dimension=768`: Matching the embedding size of the Gemini model.\n",
        "    - `metric=\"cosine\"`: For similarity-based retrieval.\n",
        "- Waits until the index is ready.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Bza_YHqHli5g"
      },
      "outputs": [],
      "source": [
        "# Initialize Pinecone\n",
        "import time\n",
        "\n",
        "index_name = \"project-2-index\"\n",
        "\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "if index_name not in existing_indexes:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
        "        time.sleep(1)\n",
        "\n",
        "index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Embedding Initialization**\n",
        "\n",
        "- GoogleGenerativeAIEmbeddings: Sets up an embedding model using Google Gemini Flash.\n",
        "- The model is identified by `model=\"models/embedding-001\"`."
      ],
      "metadata": {
        "id": "XIgFoOPdTuTO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V_S52-9uuaFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f46e2da-8990-4490-f089-9708f59d540c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05168594419956207,\n",
              " -0.030764883384108543,\n",
              " -0.03062233328819275,\n",
              " -0.02802734263241291,\n",
              " 0.01813093200325966]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vector = embeddings.embed_query(\"hello, world!\")\n",
        "vector[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Creating the Vector Store**\n",
        "\n",
        "- Uses LangChain's `PineconeVectorStore` to integrate Pinecone and the Gemini embedding model.\n",
        "- This vector store will be used to add and retrieve embedded documents."
      ],
      "metadata": {
        "id": "G0D04PZAT7GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the vector store and embeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "0ZLQL5rxCd7J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Loading the PDF**\n",
        "\n",
        "- **Purpose**: Load a PDF document for processing.\n",
        "- Uses `PyPDFLoader` to extract text from `/content/Q1 2024 Report.pdf`."
      ],
      "metadata": {
        "id": "9FefXSpzUPKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load the PDF\n",
        "loader = PyPDFLoader(\"/content/Q1 2024 Report.pdf\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "Z6x2UCGYDXdS",
        "outputId": "57c5ea56-9404-4e14-e493-5ccd399b2423"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "File path /content/Q1 2024 Report.pdf is not a valid file or url",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8e2fb4904265>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Q1 2024 Report.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, password, headers, extract_images, extraction_mode, extraction_kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0;34m\"pypdf package not found, please install it with `pip install pypdf`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             )\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         self.parser = PyPDFParser(\n\u001b[1;32m    243\u001b[0m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File path %s is not a valid file or url\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: File path /content/Q1 2024 Report.pdf is not a valid file or url"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Splitting Documents into Chunks**\n",
        "\n",
        "\n",
        "- Uses `RecursiveCharacterTextSplitter` to split the loaded document:\n",
        "  - `chunk_size=800`: Each chunk has 800 characters.\n",
        "  - `chunk_overlap=100`: 100 characters overlap between consecutive chunks."
      ],
      "metadata": {
        "id": "t5GSMqdEUuo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Split into Chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "SM-n9oEBUnUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Assigning Unique IDs**\n",
        "\n",
        "- Assigns a unique identifier to each document chunk using `uuid4`."
      ],
      "metadata": {
        "id": "da8PpXOhVBme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "# Generate Unique IDs for Each Chunk\n",
        "uuids = [str(uuid4()) for _ in range(len(docs))]"
      ],
      "metadata": {
        "id": "PSn_L-4hB3hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Adding Chunks to the Vector Store**\n",
        "\n",
        "- Adds each document chunk to the vector store along with its unique ID."
      ],
      "metadata": {
        "id": "7W9IcKpZVP9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Chunks to Vector Store\n",
        "for i, doc in enumerate(docs):\n",
        "    vector_store.add_documents(\n",
        "        documents=[doc],\n",
        "        ids=[uuids[i]],\n",
        "    )\n"
      ],
      "metadata": {
        "id": "8nYCS_65VL9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Configuring the Retriever**\n",
        "\n",
        "- Configures a retriever to fetch relevant chunks from the vector store:\n",
        "  - **Search Type**: `similarity_score_threshold`.\n",
        "  - **Search Parameters**: Retrieves top `k=5` chunks with a minimum score of `0.7`."
      ],
      "metadata": {
        "id": "tuLTi7r-VboE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"k\": 5, \"score_threshold\": 0.7},\n",
        ")\n",
        "# retriever.invoke(\"earning per share\")"
      ],
      "metadata": {
        "id": "r4xaHHopJ6u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Initializing Google Gemini LLM**\n",
        "\n",
        "- Purpose: Sets up the Gemini Flash language model for answering queries.\n",
        "- Parameters:\n",
        "  - `model='gemini-1.5-flash'`: Specifies the Gemini model.\n",
        "  - `temperature=0.7`: Controls randomness in responses."
      ],
      "metadata": {
        "id": "9Lag7cZJVs3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Configure the Google Gemini\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model = 'gemini-1.5-flash',\n",
        "    temperature = 0.7,\n",
        ")"
      ],
      "metadata": {
        "id": "eWTHJf6pFpo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Setting Up the Retrieval QA Chain**\n",
        "\n",
        "- Creates a Retrieval QA Chain:\n",
        "  - Combines the LLM and retriever.\n",
        "  - Uses `map_reduce` as the chain type.\n",
        "  - Enables verbose output for debugging."
      ],
      "metadata": {
        "id": "sS_JzDZJV_Ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"map_reduce\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "B0WRvATtQTt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **User Query and Response Display**\n",
        "\n",
        "\n",
        "- **User Query**: Asks a question about \"earnings per share in 2024.\"\n",
        "- The chain retrieves relevant document chunks, generates a response, and displays:\n",
        "  - The query.\n",
        "  - The model's answer.\n",
        "  - The source documents used to answer the query."
      ],
      "metadata": {
        "id": "QIGUagtFWMzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "# User Query\n",
        "query = \"what is earning per share in 2024\"\n",
        "response = qa_chain.invoke(query)\n",
        "\n",
        "# Display query\n",
        "print(\"---> User Query....\")\n",
        "print(response.get(\"query\"))\n",
        "\n",
        "# Display response\n",
        "print(\"---> Answer....\")\n",
        "display(Markdown(response.get(\"result\")))\n",
        "print(\"---> Source Documents....\")\n",
        "for document in response.get(\"source_documents\"):\n",
        "    print(document.metadata)\n"
      ],
      "metadata": {
        "id": "PqPNqnkVGcIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rdFbdwLxGp5I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}